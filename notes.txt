DL todo
    eval_synth voice embedding distribution visualize: remove unpitched pages

amplitude inductive bias
    at first, i thought: assume most amplitude to be 0. Like a lasso loss. 
    However, imagine: the trumpet time is the superset of violin time. then, under lasso, it would model the violin as containing the trumpet sound, and let trumpet amplitude reach 0 while violin is playing. 
    So, instead of penalizing amplitude, we should penalize amplitude change. Two ways:
        - add a loss. 
        - represent time-amplitude with neural implicit field. (let NN low-freq inductive bias do the work)

A piece is a set of instrument tracks. 

An instrument track is:
    timbre. independent with t. 
    f0(t). intermediate
    amplitude(t). requires grad
    vocal only: 2/3-dim vowel_embedding(t). requires grad

timbre is 
    energy = NIF(freq, f0, amplitude, [vowel_emb])

f0 is:
    pitch. scaler. 
    octave: 16-dim multi-hot vector. 
    
    sum the three octaves near the max in forward pass. 

About optim
    Parallel may not be important. If algo good, converge fast, few shot, then optim for GPU may be harmful and unecessary. 

vowel_embedding inductive bias
    smooth
    must add noise, before entering NIF
    实验结果：
        不需要 noise, 也不需要 smoothness regularization, 就可以：
            vowel emb 关于 t 平滑
            NIF 行为 关于 vowel emb 平滑
        NIF 神奇

merge instrument tracks. conditions:
    - amplitude-weighted f0-chroma doesn't conflict
    AND
        - amplitude is similar
        OR
        - timbre is similar
    Why is this necessary?
        just start with the right number of tracks?
        start with 1 track and gradually add tracks until loss doesn't decrease?

variational neural implicit field
    perturb the input

VENIF
    variational encoder of neural implicit fields
    inspired by wave cube synthesis. 
    what if you can stack NIFs, add VAE-like noise, and interpolate between NIFs? 
    The rationale: make different NIFs dedicate similar NN locations to related tasks. Make different NIFs robust to brain-interference. 
    Next, combine with VAE. 
    Image - encoder - reparametrize - 2d weight-bias field
    XY coord - 2d weight-bias field as NN - RGB
    It's a VAE with an oracle decoder, where the oracle decoder is an interpreter to interpret latent field as NN. 
    
    experiment
        interpolate between similar images. 
            tranlsate view?
            scale?
            same scene, diff camera?
            same object, diff style?
            swap multiple objects?

Stretched partials:
    important? no? 
    neural implicit field?

reverb:
    important? no? 

about unpitched component:
    maybe a NIF? sensitive to timing, but insensitive (add noise) to freq? 
    but maybe not important. See if the unpitched component messes up the pitched component. 

about MSE vs sum
    at first, i thourhg: 
        MSE on spectrogram means grad on per-page latents aren't normalized as batch_size changes. 
    but actually, Adam is agonistic to a linear scaling of the grad, I think 

experiments
    polyphonies
    2 vocals with instruments
    few-person acapella
    200-jacob acapella
    4-part choir
    bird chirps
        bird "vowel" spave
        source seperate

TIMELINE
5/22
    2023_m05_d19@21_21_44_not_latent
    freqNorm and eval_synth are working properly. Next, f0 is latent. 